[
  {
    "name": "Principal Component Analysis",
    "repo": "pca",
    "category_name": "降维算法",
    "difficulty": "intermediate",
    "description": "# Principal Component Analysis\n\nAn important decision to make when training machine learning models is the the features to use for the training dataset. Principal Component Analysis allows you to see which features account for most of the variance, simplifying the dataset to a smaller number of correlated variables.",
    "theory": [
      "## Target Dataset\n\nLet's generate a noisy list of points by generating points between a start and end point and by adding random noise to each point",
      "## Eigenvectors and Eigenvalues\n\nWhen you multiply a matrix with its eigenvector, you get a multiple of the eigenvector. The scalar multiple is the eigenvector's eigenvalue. The process of finding the eigenvectors and eigenvalues of a matrix is called Eigendecomposition.\n\n$$A \\vec{v} = \\lambda \\vec{v}$$\n\nThe scalar $\\lambda$ is the eigenvalue and the vector $\\vec{v}$ is the corresponding eigenvector. The eigendecomposition typically involves solving the determinant $det(A - \\lambda I) = 0$, where $I$ is the identity matrix.\n\nUse numpy to quickly get the eigenvalues and eigenvectors of a matrix\n\n```py\nimport numpy as np\nmat = np.array([[4, -2],\n                [1,  1]])\neig_vals, eig_vecs = np.linalg.eig(mat)\n```",
      "## Lagrange Multipliers (Optimization with constraints)\n\nRecall from Multivariable Calculus that Lagrange Multipliers allow you to find the extrema of a function $f(x, y, z, ...)$ that is subject to a constraint function $g(x, y, z, ...)=0$. \n\nThe Lagrange Multipliers technique states that the solution of this constrainted optimization problem is the solution to the following  system of equations:\n\n$$\\nabla L = 0$$\n\nwhere\n\n$$L(x, y, z, ... \\lambda) = f(x, y, z, ... \\lambda) - \\lambda g(x, y, z, ... \\lambda)$$",
      "## PCA Derivation (Eigendecomposition of Covariance Matrix)\n\nRecall that our goal is to find the vectors $v$ that account for most of the variance. \n\nGiven input vector $x_i$ and vector $v$, we want to project every input point to $v$ in each dimension.\n\n$$z_i = x_i^Tv$$\n\nThe variance is \n\n$$(x_i^Tv)^2 = z_i^2$$\n\nTo find the maximum variance across all of the projections for the $n$ dimensions.\n\n$\n\\begin{align*}\n\\max \\sum_{i=1}^{n} (x_i^Tv)^2 &= \\max \\sum_{i=1}^{n} z_i^2 \\\\ \n&= \\max z^Tz \\\\ \n&= \\arg\\max (xv)^Txv \\\\ \n\\end{align*}\n$\n\nSince the ratios of the Principal Components is all that matters, let's introduce the constaint that\n$$v^Tv = 1$$",
      "Solving the constrainted optimization with Lagrange Multipliers, we define the Lagrangian function: \n\n$$ L = \\arg\\max v^Tx^Txv - \\lambda (v^Tv - 1)$$\n\nLet's solve the Lagrangian function by solving $\\nabla L = 0$\n\n$\n\\begin{align*}\n0 &= \\frac{\\partial L}{\\partial v} \\\\ \n&= \\frac{\\partial}{\\partial v}[v^Tx^Txv - \\lambda (v^Tv - 1)]  \\\\ \n&= 2x^Txv - 2\\lambda v  \\\\\n&= x^Txv - \\lambda v  \\\\\n&= (x^Tx)v - \\lambda v  \\\\\n(x^Tx)v &= \\lambda v  \\\\\n\\end{align*}\n$\n\nGiven that $x^Tx$ is the covariance of a matrix, we see that the solution to PCA is simply the eigendecomposition of the covariance matrix.",
      "## PCA Implementation\n\nTo recap the two sections above, PCA consists of the following parts:\n1. Standard the input data by dividing the difference of the data and the mean by the standard deviation.\n2. Compute the covariance matrix of the standardized input\n3. Compute eigenvalues and eigenvectors of the covariance matrix\n4. To get the projected data, matrix multiply the standardized input and the eigenvectors.",
      "## Graphing Functions\n\nUtility functions to create the visualizations",
      "## Visualize PCA\n\nGiven the derivation of PCA, let's visualize the projected data with different values for the target dimension.",
      "## Scikit-Learn Implementation\n\nAs a check for correctness, let's compare our results with the PCA module from scikit-learn.\n\nNote: The sign of the values might not match exactly. They just need to have the same ratios, which they do. Our implementation matches the one from scikit-learn.",
      "Let's also plot scikit's learn projected data. Our implementation seems to match for the projected as well."
    ],
    "code": "import numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport scienceplots\n\nfrom IPython.display import Image\n\nnp.random.seed(0)\n\n# ---\n\ndef generate_noisy_hyperplane(num_points, start_pt, end_pt, noise=0.25):\n    # create a plane from the start to the end point\n    t = np.linspace(0.0 + noise, 1.0 - noise, num_points).reshape(-1, 1)\n    points = start_pt + t * (end_pt - start_pt)\n\n    # add noise to plane\n    noise = np.random.normal(0, noise, size=(num_points, 3))\n    points = points + noise\n\n    return points\n\nstart_pt = np.array([-1, -1, -1])\nend_pt = np.array([1, 1, 1])\nX = generate_noisy_hyperplane(200, start_pt, end_pt)\n\n# plot the points\n\n# ---\n\ndef pca(X, dims):\n    # subtract the mean to center the data and divide by standard deviation\n    X_centered = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n\n    # compute covariance matrix\n    cov = np.cov(X_centered.T)\n\n    # eigendecomposition of the covariance matrix\n    # the eigenvectors are the principal components\n    # the principal components are the columns of the eig_vecs matrix\n    eig_vals, eig_vecs = np.linalg.eig(cov)\n\n    # sort the eigenvalues and eigenvectors\n    sorted_idx = np.argsort(eig_vals)[::-1]\n    eig_vals = eig_vals[sorted_idx]\n    eig_vecs = eig_vecs[:, sorted_idx]\n\n    # perform dimensionality reduction using the computed principal components\n    # if you want to reduce to K dimensions, simplify take the first K columns\n    projected = X_centered @ eig_vecs\n\n    # compute the variance of each dimension (column)\n    pc_variances = [np.var(projected[:, i]) for i in range(dims)]\n\n    return eig_vals, eig_vecs, projected, pc_variances\n\n# ---\n\ndef create_plots():\n\n    ax0.set_xlabel(\"X\")\n    ax0.set_ylabel(\"Y\")\n    ax0.set_zlabel(\"Z\")\n    ax0.set_title(\"PC Hyperplanes\")\n    ax0.view_init(17, -125, 2)\n    ax0.set_xlim(-1, 1)\n    ax0.set_ylim(-1, 1)\n    ax0.set_zlim(-1, 1)\n    ax0.tick_params(axis=\"both\", which=\"both\", length=0)\n\n    ax1.set_xlabel(\"X\")\n    ax1.set_ylabel(\"Y\")\n    ax1.set_zlabel(\"Z\")\n    ax1.set_title(\"Projected Data\")\n    ax1.view_init(17, -125, 2)\n    # ax1.set_xlim(-1, 1)\n    # ax1.set_ylim(-1, 1)\n    # ax1.set_zlim(-1, 1)\n    ax1.tick_params(axis=\"both\", which=\"both\", length=0)\n\n    return ax0, ax1, camera\n\ndef plot_hyperplane(ax, pc_vector, color=\"red\", scaling=10, alpha=0.3):\n    # Create a grid of points\n    points = np.linspace(-1, 1, scaling)\n    xx, yy = np.meshgrid(points, points)\n\n    # the z value is the defined by the hyperplane from the principal component vector\n    pc_vector /= np.linalg.norm(pc_vector)\n    z = (-pc_vector[0] * xx - pc_vector[1] * yy) / pc_vector[2]\n\n# ---\n\ndef visualize_pca(X, dims, output_filename):\n    ax0, ax1, camera = create_plots()\n    colors = [\"red\", \"green\", \"blue\"]\n\n    for dim in range(0, dims + 1):\n        eig_vals, eig_vecs, projected, pc_variances = pca(X, dims)\n\n        # plot the original data\n        ax0.scatter(X[:, 0], X[:, 1], X[:, 2], color=\"blue\", label=\"Original Data\")\n\n        # plot the pca hyperplanes\n        for i in range(dim):\n            plot_hyperplane(ax0, eig_vecs[:, i], color=colors[i])\n\n        # plot the projected data from the principal components\n        curr_projected = projected\n        for i in range(dim, dims):\n            if i < dims:\n                curr_projected[:, i] = 0\n        if dim != 0:\n            ax1.scatter(\n                curr_projected[:, 0],\n                curr_projected[:, 1],\n                curr_projected[:, 2],\n                color=\"blue\",\n                label=\"Projected Data\",\n            )\n\n    eig_vals, eig_vecs, projected, pc_variances = pca(X, dims)\n\n    print(\"variance percentage per principal component\")\n    variance_percentage = eig_vals / np.sum(eig_vals)\n    for i, percentage in enumerate(variance_percentage):\n        print(f\"{i+1}th PC: {round(percentage*100, 2)}%\")\n\n    print(\"variance per principal component\")\n    for i, variance in enumerate(pc_variances):\n        print(f\"{i+1}th PC: {round(variance, 2)}\")\n\n    print(\"\\nhyperplanes\")\n    for i in range(dim):\n        print(f\"hyperplane {i}: {eig_vecs[:, i]}\")\n\n# ---\n\ndims = 3\noutput_filename = \"pca.gif\"\n\nvisualize_pca(X, dims, output_filename)\n\n# ---\n\nImage(filename=output_filename)\n\n# ---\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_centered = scaler.fit_transform(X)\n\npca = PCA(n_components=dims)\nprojected = pca.fit_transform(X_centered)\neig_vecs = pca.components_\n\nprint(\"\\nhyperplanes\")\nfor i in range(dims):\n    print(f\"hyperplane {i}: {eig_vecs[i]}\")",
    "tags": [],
    "visualization_data": {},
    "papers": [],
    "resources": []
  },
  {
    "name": "Logistic Regression",
    "repo": "logistic-regression",
    "category_name": "线性模型",
    "difficulty": "intermediate",
    "description": "# Logistic Regression\n\nBinary Classification model that finds the optimal the weights and bias and returns probabilites of the two classes",
    "theory": [
      "## Training Dataset\n\nLet's import the breast cancer dataset. The logistic regression will perform binary classification using the mean perimeter and mean radius of the tumor.",
      "## Activation Function\n\nRecall that the output of the perceptron was the dot product between the weight vector $\\vec{w}$ and the input vector $\\vec{x}$ plus a constant bias term $b$\n\nPerceptron: $y=w^Tx + b$\n\nActivation functions are applied after the computation.",
      "### Sigmoid Function\n\nIn order to do binary classification, we would like to limit the value of the output to be in the range (0, 1) and get a value to represent the probability of the output being assigned to either class. The sigmoid function is perfect for this\n\n$\n\\sigma(z) = \\frac{1}{1+e^-z}\n$",
      "## Gradient of Sigmoid Function\n\nGradient Descent will be used later to find the optimal weight values. As a result, let's calculate the gradient of the sigmoid function.",
      "$\n\\begin{align*}\n\\sigma^\\prime\n&= \\frac{\\partial}{\\partial z} \\sigma(z) \\\\\n&= \\frac{\\partial}{\\partial z} (\\frac{1}{1+e^{-z}}) \\\\\n&= \\frac{\\partial}{\\partial z} (1+e^{-z})^{-1}) \\\\\n&= (-1)(1+e^{-z})^{-2}\\frac{\\partial}{\\partial z}(1+e^{-z}) \\\\\n&= (-1)(1+e^{-z})^{-2}(e^{-z})\\frac{\\partial}{\\partial z}(-z) \\\\\n&= (-1)(1+e^{-z})^{-2}(e^{-z})(-1) \\\\\n&= \\frac{e^{-z}}{(1+e^{-z})^{2}}\n\\end{align*}\n$",
      "## Autograd\nAlternatively, you can use autograd to differentiate a numpy function. Pytorch and JAX also implement autograd.",
      "## Loss Function\n\nThe binary cross entropy loss function will be used for logistic regression. This loss function is derived from the definition of maximum likelihood estimation.\n\nFor the binary classification model, the probability of seeing the first class is the sigmoid activation function is applied over the sum of the bias $b$ and the dot product of the weight vector $\\vec{w}$ and the input vector $\\vec{x}$. The probability of seeing the other class is the difference between 1 and the probability of seeing the other class.\n\n$\nP(Y=1 \\mid \\vec{x}; \\vec{w}, b) = \\sigma{(\\vec{w} \\cdot \\vec{x} + b)}\n$\n\n$\nP(Y=0 \\mid \\vec{x}; \\vec{w}, b) = 1 - P(Y=1 \\mid \\vec{x}; \\vec{w}, b)\n$",
      "### Maximum Likelihood Estimation\n\nMaximum likelihhod estimation finds the optimal weights and bias to maximize the probability of seeing the training data.\n\nThe probability of getting the correct binary prediction function in terms of $\\vec{w}$ and $b$ is the following. This can also be thought of as a Bernoulli distribution.\n\n$\nP(Y \\mid \\vec{x}; \\vec{w}, b) = [\\sigma{(\\vec{w} \\cdot \\vec{x} + b)}]^{y} [1 - \\sigma{(\\vec{w} \\cdot \\vec{x} + b)}]^{1-y}\n$\n\nWith a training dataset of $i$ examples of $\\vec{x_i}$ features and $y_i$ labels, so the total probability is written as the product of the probabilites of all the training examples. Consider this as the likelihood of the training dataset with the current weights and bias.\n\n$\nP(Y \\mid \\vec{x_i}; \\vec{w}, b) = \\prod_{i=1}^{n} [\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{y_i} [1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{1-y_i}\n$\n\nKeep in mind that we want to find the set of optimal parameters $\\vec{w}$ and $b$ that maximize the total likelihood.\n\n$\nP(Y \\mid \\vec{x_i}; \\vec{w}, b) = \\max_{\\vec{w}, b} \\prod_{i=1}^{n} [\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{y_i} [1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{1-y_i}\n$",
      "In order to find the optimal weights and bias for the logistic regression model, we use gradient descent, which is a solution to optimization problems. We have to take the partial derivative of the likelihood with respect to $\\vec{w}$ and $b$. \n\nIn it's current form, the total probability is a lot of multiplications. Per the product rule for derivatives, the partial derivatives will also be a lot of multiplication. In order to avoid this, we can take the logarithm of the likelihood, which converts the multiplications into additions.\n\n$\n\\begin{align*}\n\\ln(P(Y \\mid \\vec{x_i}; \\vec{w}, b)) &= \\max_{\\vec{w}, b} \\ln(\\prod_{i=1}^{n} [\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{y_i} [1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{1-y_i}) \\\\\n&= \\max_{\\vec{w}, b} \\sum_{i=1}^{n}[ \\ln([\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{y_i} [1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{1-y_i})] \\\\\n&= \\max_{\\vec{w}, b} \\sum_{i=1}^{n}[ \\ln([\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{y_i}) + \\ln([1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}]^{1-y_i})] \\\\\n&= \\max_{\\vec{w}, b} \\sum_{i=1}^{n}[ y_i \\ln(\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}) + (1-y_i) \\ln(1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)})] \\\\\n\\end{align*}\n$\n\nWe define the negative (multiply equation above by -1) log of the likelihood as the binary cross entropy loss function. Let's also divide by the number of training examples to make this the average loss across the $n$ examples.\n\n$\n\\begin{align*}\nL(\\vec{w}, b) = -\\frac{1}{n}  \\sum_{i=1}^{n}[ y_i \\ln(\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}) + (1-y_i) \\ln(1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)})] \n\\end{align*}\n$",
      "### Binary Cross Entropy Loss Function\n\nRecall that $\\hat y = \\sigma{(\\vec{w} \\cdot \\vec{x} + b)} = \\frac{1}{1+e^-(\\vec{w} \\cdot \\vec{x} + b)}$\n\nTo simplify the calculation of the loss, let's rewrite it in terms of $\\hat y$\n\n$\n\\begin{align*}\nL(\\hat y) &= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\ln(\\hat y_i) + (1-y_i) \\ln(1 - \\hat y_i)]\n\\end{align*}\n$",
      "## Loss Function in Terms of W and B\n\nBefore taking the partial derivative of the loss function with respect to $\\vec{w}$ and $b$. Let's simplify it to make the partial derivative calculaton easier.\n\n$\\hat y = \\sigma{(\\vec{w} \\cdot \\vec{x} + b)} = \\frac{1}{1+e^-(\\vec{w} \\cdot \\vec{x} + b)}$\n\n$\n\\begin{align*}\nL(\\vec{w}, b) &= -\\frac{1}{n}  \\sum_{i=1}^{n}[ y_i \\ln(\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}) + (1-y_i) \\ln(1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)})] \\\\ \n&= -\\frac{1}{n}  \\sum_{i=1}^{n}[ y_i \\ln(\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}) - y_i \\ln(1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}) + \\ln(1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)})] \\\\ \n&= -\\frac{1}{n}  \\sum_{i=1}^{n}[ y_i \\ln(\\frac{\\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}}{1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)}}) + \\ln(1 - \\sigma{(\\vec{w} \\cdot \\vec{x_i} + b)})] \\\\\n&= -\\frac{1}{n}  \\sum_{i=1}^{n}[ y_i \\ln(\\frac{\\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x} + b)}}}{1 - \\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x} + b)}}}) + \\ln(1 - \\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x} + b)}})] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\ln(e^{\\vec{w} \\cdot \\vec{x_i} + b}) + \\ln(\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}})] \\\\ \n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i (\\vec{w} \\cdot \\vec{x_i} + b) - \\ln(1+e^{\\vec{w} \\cdot \\vec{x_i} + b})] \\\\ \n\\end{align*}\n$",
      "## Loss Function Gradient",
      "Loss function with respect to $W$\n \n$\n\\begin{align*}\n\\nabla_{W} [L(\\vec{w}, b)] &= \\nabla_{W} [-\\frac{1}{n} \\sum_{i=1}^{n} [y_i (\\vec{w} \\cdot \\vec{x_i} + b) - \\ln(1+e^{\\vec{w} \\cdot \\vec{x_i} + b})]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\nabla_{W} [\\vec{w} \\cdot \\vec{x_i} + b] - \\nabla_{W}[\\ln(1+e^{\\vec{w} \\cdot \\vec{x_i} + b})]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i x_i - (\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}}) \\nabla_{W}[1+e^{\\vec{w} \\cdot \\vec{x_i} + b}]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i x_i - (\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}}) (e^{\\vec{w} \\cdot \\vec{x_i} + b})\\nabla_{W}[\\vec{w} \\cdot \\vec{x_i} + b]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i x_i - (\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}}) (e^{\\vec{w} \\cdot \\vec{x_i} + b})(x_i)] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i x_i - x_i(\\frac{e^{\\vec{w} \\cdot \\vec{x_i} + b}}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}})] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i x_i - x_i(\\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x_i} + b)}})] \\\\ \n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i x_i - x_i\\hat y_i] \\\\ \n&= \\frac{1}{n} \\sum_{i=1}^{n} [\\hat y_i x_i - y_i x_i  ] \\\\ \n&= \\frac{1}{n} \\sum_{i=1}^{n} [x_i (\\hat y_i - y_i)] \\\\\n\\end{align*}\n$",
      "Loss function with respect to $b$\n\n$\n\\begin{align*}\n\\frac{\\partial}{\\partial b}  [L(\\vec{w}, b)] &= \\frac{\\partial}{\\partial b} [-\\frac{1}{n} \\sum_{i=1}^{n} [y_i (\\vec{w} \\cdot \\vec{x_i} + b) - \\ln(1+e^{\\vec{w} \\cdot \\vec{x_i} + b})]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\frac{\\partial}{\\partial b} [\\vec{w} \\cdot \\vec{x_i} + b] - \\frac{\\partial}{\\partial b}[\\ln(1+e^{\\vec{w} \\cdot \\vec{x_i} + b})]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i (1) - (\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}}) \\frac{\\partial}{\\partial b}[1+e^{\\vec{w} \\cdot \\vec{x_i} + b}] ] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i - (\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}}) (e^{\\vec{w} \\cdot \\vec{x_i} + b}) \\frac{\\partial}{\\partial b}[\\vec{w} \\cdot \\vec{x_i} + b]] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i - (\\frac{1}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}}) (e^{\\vec{w} \\cdot \\vec{x_i} + b}) (1)] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i - (\\frac{e^{\\vec{w} \\cdot \\vec{x_i} + b}}{1+e^{\\vec{w} \\cdot \\vec{x_i} + b}})] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i - (\\frac{1}{1+e^-({\\vec{w} \\cdot \\vec{x_i} + b})})] \\\\\n&= -\\frac{1}{n} \\sum_{i=1}^{n} [y_i - \\hat y_i] \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} (\\hat y_i - y_i) \\\\\n\\end{align*}\n$",
      "## Gradient Descent\n\nWith the binary cross entropy functions with respect to $\\vec{w}$ and $b$, the gradient descent equations are:",
      "Gradient Descent for weights $\\vec{w}$\n\n$\n\\begin{align*}\n\\vec{w} &= \\vec{w} - \\eta \\nabla_{W} [L(\\vec{w}, b)] \\\\\n&= \\vec{w} - \\eta [\\frac{1}{n} \\sum_{i=1}^{n} [x_i (\\hat y_i - y_i)]]\n\\end{align*}\n$",
      "Gradient Descent for bias $b$\n\n$\n\\begin{align*}\nb &= b - \\eta \\frac{\\partial}{\\partial b} [L(\\vec{w}, b)] \\\\\n&= b - \\eta [\\frac{1}{n} \\sum_{i=1}^{n} (\\hat y_i - y_i)]\n\\end{align*}\n$",
      "## Graphing functions\n\nUtility functions to create the visualizations",
      "## Training the model"
    ],
    "code": "# import numpy as np\nimport autograd.numpy as np\nfrom autograd import grad\nfrom autograd import elementwise_grad as egrad\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport sklearn.datasets as skdatasets\n\nimport scienceplots\nfrom IPython.display import Image\n\nnp.random.seed(0)\n\n# ---\n\ndataset = skdatasets.load_breast_cancer()\n\nfeatures_used = [-3, -8]\nX = dataset.data[:, features_used]\nfeature_names = dataset.feature_names[features_used]\n\n# min-max normalize the features along the columns\nX_min_vals = X.min(axis=0)\nX_max_vals = X.max(axis=0)\nX = (X - X_min_vals) / (X_max_vals - X_min_vals)\n\nY = dataset.target\ntarget_names = dataset.target_names\n\n# ---\n\nsigmoid = lambda x: 1 / (1 + np.exp(-x))\n\n# ---\n\ndef plot(fx, x_min, x_max, points=100, title=\"\"):\n\n    x = np.linspace(x_min, x_max, points)\n    y = fx(x)\n\n    axes.plot(x, y)\n    axes.set_xlabel(\"X\")\n    axes.set_ylabel(\"Y\")\n    axes.set_title(\"Activation Function\")\n\nx_min = -10\nx_max = 10\npoints = 100\nplot(sigmoid, x_min, x_max, points)\n\n# ---\n\nsigmoid_prime = lambda x: np.exp(-x) / np.power((1 + np.exp(-x)), 2)\nplot(sigmoid_prime, x_min, x_max, points)\n\n# ---\n\n# grad() differentiates scalar inputs\nsigmoid_prime_grad = grad(sigmoid)\n# egrad() differentiates vectorized inputs\nsigmoid_prime_egrad = egrad(sigmoid)\n\nx = np.linspace(x_min, x_max, points)\nassert sigmoid_prime_grad(x[0]) == sigmoid_prime(x[0])\nassert np.allclose(sigmoid_prime_egrad(x), sigmoid_prime(x))\n\nplot(sigmoid_prime_egrad, x_min, x_max, points)\n\n# ---\n\ndef bce(y_true, y_pred):\n    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n# ---\n\ndef bce_dw(x, y_true, y_pred):\n    return np.mean(x * (y_pred - y_true))\n\n# ---\n\ndef bce_db(y_true, y_pred):\n    return np.mean(y_pred - y_true)\n\n# ---\n\ndef gradient_descent(weights, x, bias, y_true, y_pred, learning_rate):\n    weights = weights - learning_rate * bce_dw(x, y_true, y_pred)\n    bias = bias - learning_rate * bce_db(y_true, y_pred)\n\n    return weights, bias\n\n# ---\n\ndef create_plots():\n\n    ax[0].set_xlabel(\"Epoch\", fontweight=\"normal\")\n    ax[0].set_ylabel(\"Error\", fontweight=\"normal\")\n    ax[0].set_title(\"Binary Cross Entropy Error\")\n\n    ax[1].axis(\"off\")\n    ax[2].axis(\"off\")\n\n    ax[2].set_xlabel(\"X\")\n    ax[2].set_ylabel(\"Y\")\n    ax[2].set_zlabel(\"Z\")\n    ax[2].set_title(\"Prediction Probabilities\")\n    ax[2].view_init(20, -35)\n\n    return ax[0], ax[2], camera\n\n    ax0,\n    ax1,\n    idx,\n    visible_mse,\n    mse_idx,\n    errors,\n    features,\n    labels,\n    predictions,\n    points_x,\n    points_y,\n    surface_predictions,\n):\n    ax0.plot(\n        mse_idx[visible_mse][: idx + 1],\n        errors[visible_mse][: idx + 1],\n        color=\"red\",\n        alpha=0.5,\n    )\n\n    # Plot Logistic Regression Predictions\n    # Ground truth and training data\n    ground_truth_legend = ax1.scatter(\n        features[:, 0],\n        features[:, 1],\n        labels,\n        color=\"red\",\n        alpha=0.5,\n        label=\"Ground Truth\",\n    )\n    # Logistic Regression Predictions\n    predictions_legend = ax1.scatter(\n        features[:, 0],\n        features[:, 1],\n        predictions,\n        color=\"blue\",\n        alpha=0.2,\n        label=\"Prediction\",\n    )\n        points_x,\n        points_y,\n        surface_predictions.reshape(dims, dims),\n        color=\"blue\",\n        alpha=0.2,\n    )\n    ax1.legend(\n        (ground_truth_legend, predictions_legend),\n        (\"Ground Truth\", \"Predictions\"),\n        loc=\"upper left\",\n    )\n\n# ---\n\ndef fit(\n    w0, b0, features, labels, dims, epochs, learning_rate, optimizer, output_filename\n):\n    mse_idx = np.arange(1, epochs + 1)\n    errors = np.full(epochs, -1)\n    ax0, ax1, camera = create_plots()\n\n    points = np.linspace(0, 1, dims)\n    points_x, points_y = np.meshgrid(points, points)\n    surface_points = np.column_stack((points_x.flatten(), points_y.flatten()))\n\n    weights = w0\n    bias = b0\n\n    for idx in range(epochs):\n        error = 0\n        predictions = np.array([])\n        surface_predictions = np.array([])\n\n        # fit the model on the training data\n        for x, y in zip(features, labels):\n            output = sigmoid(np.dot(weights, x) + bias)\n\n            predictions = np.append(predictions, output)\n\n            # Store Error\n            error += bce(y, output)\n\n            # Gradient Descent\n            weights, bias = optimizer(weights, x, bias, y, output, learning_rate)\n\n        # error /= len(features)\n\n        # Visualization\n        if (\n            idx < 5\n            or (idx < 15 and idx % 5 == 0)\n            or (idx <= 50 and idx % 25 == 0)\n            or (idx <= 1000 and idx % 200 == 0)\n            or idx % 500 == 0\n        ):\n            for surface_point in surface_points:\n                output = sigmoid(np.dot(weights, surface_point) + bias)\n                surface_predictions = np.append(surface_predictions, output)\n\n            print(f\"epoch: {idx:>4}, BCA: {round(error, 2)}\")\n\n            # Plot BCE\n            errors[idx] = error\n            visible_mse = errors != -1\n\n                ax0,\n                ax1,\n                idx,\n                visible_mse,\n                mse_idx,\n                errors,\n                features,\n                labels,\n                predictions,\n                points_x,\n                points_y,\n                surface_predictions,\n            )\n\n# ---\n\nepochs = 5001\nlearning_rate = 0.0005\ndims = 10\n\nw0 = np.random.rand(X[0].shape[0])\nb0 = np.random.rand()\n\noutput_filename = \"logistic_regression.gif\"\nfit(w0, b0, X, Y, dims, epochs, learning_rate, gradient_descent, output_filename)\n\n# ---\n\nImage(filename=output_filename)",
    "tags": [],
    "visualization_data": {},
    "papers": [],
    "resources": []
  },
  {
    "name": "Neural Network",
    "repo": "neural-network",
    "category_name": "深度学习",
    "difficulty": "advanced",
    "description": "# Neural Network\n\nNeural Networks are a machine learning model that learns the optimal parameters to approximate complex functions.\n\nGitHub Repo: https://github.com/gavinkhung/neural-network",
    "theory": [
      "Import the libraries",
      "## Training Dataset\n\nThe neural network will learn the parameters to fit a Hyperbolic Paraboloid.\n\n$\n\\begin{align*}\nz &= \\frac{y^2}{b^2} - \\frac{x^2}{a^2}\n\\end{align*}\n$",
      "## Loss Function\nThe loss function is needed to evaluate the performance of the model and to update the weights accordingly. The optimizaton process of the neural network training will find the weights and biases to minimie the loss.",
      "### Mean Squared Error\nQuadratic loss functions, like mean squared error, are used for regression tasks, like this example.\n\n$ J = \\frac{1}{n} \\sum_{i=1}^{n}(y_{i}-\\hat{y})^2 $",
      "### Mean Squared Error Gradient\nIn order to perform backpropagation to update the weights, we need to calculate the gradient of the loss function.\n\n$\n\\begin{align*}\nJ^{\\prime} &= \\frac{\\partial}{\\partial \\hat{y}} [ \\frac{1}{n} \\sum_{i=1}^{n}(y_{i}-\\hat{y})^2 ] \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\hat{y}} [ (y_{i}-\\hat{y})^2 ] \\\\\n&= \\frac{2}{n} \\sum_{i=1}^{n} (y_{i}-\\hat{y}) \\frac{\\partial}{\\partial \\hat{y}}[y_{i}-\\hat{y}] \\\\\n&= \\frac{2}{n} \\sum_{i=1}^{n} (y_{i}-\\hat{y}) (-1) \\\\\n&= \\frac{2}{n} \\sum_{i=1}^{n}(\\hat{y}-y_{i})\n\\end{align*}\n$",
      "## Neural Network Layer\n\nWe will implement a feedforward neural network, which contains many layers. Each layer at a very high applies a linear transformation to the input data to create the output data, which is often in a different dimension than the input data. Then, all of the values in the output are run through a function, known as an activation function. \n\nRecall from Linear Algebra that an input column vector $x$ in $\\mathbb{R}^n$ can be transformed into another dimension $\\mathbb{R}^m$ by multipying it with a matrix of size $m \\times n$. Finally we can add a bias term to this linear transformation to shift the transformed data up or down.\n\nAs a result, each layer stores a weight matrix and a bias vector to apply the linear transformation.",
      "## Forward Propagation\n\nThis the process of transforming our input data into the predictions from our feedforward neural network. The input data is passed through every layer of the network. Below is a visualization of the 3 layer network we will implement.\n\n![Neural Network](plot/nn.svg)",
      "Each neuron in a layer is a weighted sum of its inputs plus a bias term. Finally, an activation function is applied on each neuron in the layer. We will have a class to represent a fully-connected layer and another class to represent an activation function.\n\nThe computation for each neuron in a layer being a weighted sum of the products between the inputs and the layer's weights plus a bias term is the same as a matrix multiplication between the weight matrix and the input data added with a bias vector.\n\nThus, the foward propagation of a fully connected layer can be written as:\n\n$\n\\begin{align*}\nZ &= W X + B \\\\\nA &= g(Z)\n\\end{align*}\n$\n\nWhere:\n- $W$ is the weight matrix for the layer\n- $X$ is the input data\n- $B$ is a bias vector\n- $g$ is the activation function",
      "## Our Network's Forward Propagation",
      "We will implement a 3-layer fully-connected neural network with a Tanh activation function after each layer. These transformations can be written as these matrix multiplications:\n\n$\n\\begin{align*}\nZ_1 &= W_1 X + B_1 \\\\\nA_1 &= tanh(Z_1) \\\\\n\\\\\nZ_2 &= W_2 A_1 + B_2 \\\\\nA_2 &= tanh(Z_2) \\\\\n\\\\\nZ_3 &= W_3 A_2 + B_3 \\\\\n\\hat{y} &= A_3 = tanh(Z_3)\n\\end{align*}\n$",
      "## Backpropagation\n\nBackpropagation is the process of updating all of the weights and biases in a neural network using the chain rule and gradient descent.\n\nThe following equations use $J$ as our loss function. In this Notebook, we use the mean squared error loss function. Click [here for the mean squared error loss function](#mean-squared-error).\n\nOur goal is to apply gradient descent on the weights and bias using the following equations:\n\n$\n\\begin{align*}\nW_1 &= W_1 - lr * \\frac{\\partial J}{\\partial W_1} \\\\\nB_1 &= B_1 - lr * \\frac{\\partial J}{\\partial B_1} \\\\\nW_2 &= W_2 - lr * \\frac{\\partial J}{\\partial W_2} \\\\\nB_2 &= B_2 - lr * \\frac{\\partial J}{\\partial B_2} \\\\\nW_3 &= W_3 - lr * \\frac{\\partial J}{\\partial W_3} \\\\\nB_3 &= B_3 - lr * \\frac{\\partial J}{\\partial B_3} \\\\\n\\end{align*}\n$\n\nWe need to use the chain rule to get the values for $\\frac{\\partial J}{\\partial W_1}$, $\\frac{\\partial J}{\\partial B_1}$, $\\frac{\\partial J}{\\partial W_2}$, $\\frac{\\partial J}{\\partial B_2}$, $\\frac{\\partial J}{\\partial W_3}$, and $\\frac{\\partial J}{\\partial B_3}$.",
      "### Chain Rule for Backpropagation\n\nLet's derive a general formula to update the weight matrix $W_i$ and bias vector $B_i$ of a single fully-connected layer.\n\nTo apply gradient descent on the 3rd layer, for example, we need to find the loss with respect to $W_3$ and $B_3$, which are $\\frac{\\partial J}{\\partial W_3}$, and $\\frac{\\partial J}{\\partial B_3}$.\n\nThe loss function $J$ is in terms of $A_3$, which is the final output/activation of the last layer. Then, $A_3$ is in terms of $Z_3$. Lastly, $Z_3$ is in terms of $W_3$, $B_3$, and $A_2$, which is the activation from the second to last layer. Let's get the loss with respect to $W_3$ and $B_3$.\n\nWe can represent these matrix operations as the following composite functions: $J(A_3)$, $A_3(Z_3)$, and $Z_3(W_3, B_3, A_2)$\n\nClick [here for the operations of each layer](#our-network-s-forward-propagation).",
      "### Chain Rule For Weight Matrix W\n\nUse the chain rule to derive $\\frac{\\partial J}{\\partial W_3}$\n\n$\n\\begin{align*}\n\\frac{\\partial J}{\\partial W_3} &= \\frac{\\partial}{\\partial W_3}[J(A_3(Z_3(W_3, B_3, A_2)))] \\\\\n&= \\frac{\\partial J}{\\partial A_3} \\frac{\\partial}{\\partial W_3}[A_3(Z_3(W_3, B_3, A_2))] \\\\\n&= \\frac{\\partial J}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial}{\\partial W_3}[Z_3(W_3, B_3, A_2))] \\\\\n&= \\frac{\\partial J}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial W_3} \\\\\n\\end{align*}\n$",
      "### Chain Rule For Bias Vector B\n\nUse the chain rule to derive $\\frac{\\partial J}{\\partial B_3}$.\n\n$\n\\begin{align*}\n\\frac{\\partial J}{\\partial B_3} &= \\frac{\\partial}{\\partial B_3}[J(A_3(Z_3(W_3, B_3, A_2)))] \\\\\n&= \\frac{\\partial J}{\\partial A_3} \\frac{\\partial}{\\partial B_3}[A_3(Z_3(W_3, B_3, A_2))] \\\\\n&= \\frac{\\partial J}{\\partial A_3} \\frac{\\partial A_3}{\\partial B_3} \\frac{\\partial}{\\partial B_3}[Z_3(W_3, B_3, A_2))] \\\\\n&= \\frac{\\partial J}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial B_3} \\\\\n\\end{align*}\n$",
      "## Backpropagation for Weight Matrix W\n\n$\n\\begin{align*}\n\\frac{\\partial J}{\\partial W_3} = \\frac{\\partial J}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial W_3}\n\\end{align*}\n$\n\nLet's break each component down:\n\n1. $\\frac{\\partial J}{\\partial A_3}$ is the gradient of the loss function, which is the gradient of the mean squared error function. Click [here for the derivation of the loss function gradient](#mean-squared-error-gradient). In the general case for any layer, this is the gradient from the next layer (idx+1).\n\n2. $\\frac{\\partial A_3}{\\partial Z_3} = \\frac{\\partial}{\\partial Z_3}[tanh(Z_3)] $ is the gradient of the activation function. Click [here for the derivation of the activation function gradient](#tanh-activation-function-gradient).\n\n3. $\\frac{\\partial Z_3}{\\partial W_3} = \\frac{\\partial}{\\partial W_3}[W_3 A_2 + B_3] = A_2$ is the original input to the layer, which is the output of the previous layer (idx-1).\n\nAs a result, to the gradient of the weight matrix of a fully-connected layer is the matrix multiplication products of the following:\n1. The gradient from the next layer (idx+1)\n2. The gradient of the activation function\n3. The input from the previous layer (idx-1)",
      "## Backpropagation for Bias Vector B\n\n$\n\\begin{align*}\n\\frac{\\partial J}{\\partial B_3} = \\frac{\\partial J}{\\partial A_3} \\frac{\\partial A_3}{\\partial Z_3} \\frac{\\partial Z_3}{\\partial B_3}\n\\end{align*}\n$\n\nLet's break each component down: \n\n1. $\\frac{\\partial J}{\\partial A_3}$ is the gradient of the loss function, which is the gradient of the mean squared error function. Click [here for the derivation of the loss function gradient](#mean-squared-error-gradient). In the general case for any layer, this is the gradient from the next layer (idx+1).\n\n2. $\\frac{\\partial A_3}{\\partial Z_3}$ is the gradient of the activation function. Click [here for the derivation of the activation function gradient](#tanh-activation-function-gradient).\n\n3. $\\frac{\\partial Z_3}{\\partial B_3} = \\frac{\\partial}{\\partial B_3}[W_3 A_2 + B_3] = 1$ is 1, we can ignore this in the gradient computation.\n\nAs a result, to the gradient of the bias vector of a fully-connected layer is the matrix multiplication products of the following:\n1. The gradient from the next layer (idx+1)\n2. The gradient of the activation function",
      "For more information, I recommend the follow resources:\n- [Neural Network from Scratch](https://www.youtube.com/watch?v=pauPCy_s0Ok). I also watched this video to help write this Notebook.\n- [The Most Important Algorithm in Machine Learning](https://www.youtube.com/watch?v=SmZmBKc7Lrs)",
      "## Dense Layers\n\nA dense layer is a fully connected layer. Let's use the equations derived in the forward and backwards propagation sections above to implement the `forward()` and `backward()` methods of our dense layer class.",
      "## Activation Function\n\nActivation functions are applied after the matrix multiplication to introduce nonlinearity into our neural networks. Choosing the correct activation function is essential for the neural network to learn general patterns of the training data. Most notably, the ReLU function is very useful when training very deep neural networks with many layers, as seen from the 2012 AlexNet paper, in order to prevent vanishing gradients, where the network fails to update its weights from super small gradients",
      "### Tanh Activation Function\n\nWe will use the Tanh activation function for our network:\n\n$\n\\begin{align*}\n\\sigma(z) &= \\frac{e^z-e^{-z}}{e^z+e^{-z}}\n\\end{align*}\n$",
      "### Tanh Activation Function Gradient\n\nSince gradient descent relies on knowing the gradient of our activation function, let's derivate the gradient of the tanh function.\n\n$\n\\begin{align*}\n\\sigma^\\prime(z) &= \\frac{\\partial}{\\partial z} [ \\frac{e^z-e^{-z}}{e^z+e^{-z}} ] \\\\\n&= \\frac{(e^z+e^{-z})\\frac{\\partial}{\\partial z}[e^z-e^{-z}] - (e^z-e^{-z})\\frac{\\partial}{\\partial z}[e^z+e^{-z}]}{({e^z+e^{-z}})^2} \\\\\n&= \\frac{(e^z+e^{-z})(e^z+e^{-z}) - (e^z-e^{-z})(e^z-e^{-z})}{({e^z+e^{-z}})^2} \\\\\n&= \\frac{(e^z+e^{-z})^2 - (e^z-e^{-z})^2}{({e^z+e^{-z}})^2} \\\\\n&= \\frac{(e^z+e^{-z})^2}{({e^z+e^{-z}})^2} - \\frac{(e^z-e^{-z})^2}{({e^z+e^{-z}})^2} \\\\\n&= 1 - [\\sigma(z)]^2\n\\end{align*}\n$",
      "Now that we have derived the gradient of the Tanh function, let's create the class for the Tanh activation function.",
      "## Optimizer\n\nOur optimization algorithm will be Gradient Descent, allowing us to determine how to update the parameters in the next iteration.\n\n$ X_{n+1} = X_n - lr * \\frac{\\partial}{\\partial X} f(X_n)$.\n\nLet's create a class that updates the weight matrix and the bias vector using the Gradient Descent equation.",
      "## Graphing Functions\n\nThis Notebook will create many visualizations of the neural network during its training phase.\n\n`create_scatter_and_3d_plot()` creates a 2 column graph for the Mean Squared Error graph on the left and a 3D graph on the right.\n\n`create_3d_and_3d_plot()` creates a 2 column graph with two 3D graphs.\n\n`plot_3d_predictions()` plots the neural network's current preditions and the expected output of the neural network.\n\n`plot_layer_loss_landscape()` plots how close one layer's current weights are from the optimal weights by seeing how the total mean squared error changes if the weights were shifted a little.",
      "## Training the model\n\nLet's tie everything together to train the neural network. In each epoch, we will do the following:\n\n1. Pass the training data into our model to get the model's predictions\n2. Calculate the loss from the model's predictions and the expected value\n3. Use the loss to run the optimizer to update the weights and biases\n4. Call the visualization functions to visualize the training process",
      "The model can be visualized with the following:\n\n<img src=\"plot/architecture/nn-1.png\" alt=\"Fully Connected Neural Network\" width=\"400\" />",
      "Our model consists of 3 layers with the Tanh() activation function after each layer.\n\nLayer 1:\n- Input Dimensions: 2\n- Output Dimensions: 12\n\nLayer 2:\n- Input Dimensions: 12\n- Output Dimensions: 2\n\nLayer 3:\n- Input Dimensions: 2\n- Output Dimensions: 1",
      "Let's train our model by passing our model and optimizer to our training method",
      "## Output GIF\n\nIn this visualization, we see the predictions fit the ground truth. The neural network was able to find the optimal parameters to fit this function. Now think about the applications of this. Given input data about people's shopping habits, we can predict things to recommend to them. We can recommend a social media post to show them or a show to watch next. We can pass data to neural networks and it will uncover patterns from input data and find patterns.",
      "The visualization below shows that backpropagation finds the optimal weights for the neural network. On the left graph, the red dot shows the current values of the weight matrix in the last layer of the neural network. The z axis is the mean squared error loss. If the weights weren't at the current value, the loss wouldn't be at a minima, meaning that backpropagation in fact does update the weights to the most optimal (local extrema) value and allows the network to converge.",
      "## Pytorch Implementation\n\nMachine Learning libraries, such as PyTorch, provide utilities to easily train and test neural networks on all types of optimized hardware. Now we will implement the same network in PyTorch.",
      "## PyTorch nn.Module\n\nWe represent our neural network by creating a subclass of the `nn.Module` PyTorch class that defines all of the layers in the network and how data flows through the network. The `forward()` method is the forward propagation of our neural forward. Notice that we don't need to specify anything for the backpropagation process. PyTorch takes care of this for us using their auto-differentiation support. We just need to import an optimizer class, like the PyTorch provided gradient descent class, and pass in our model's parameters. This is the beauty of using machine learning libraries.",
      "## PyTorch Training\n\nSimilar to our own implementation, let's use our model to define our training process. In each epoch, we will do the following:\n\n1. Pass the training data into our model to get the model's predictions\n2. Calculate the loss from the model's predictions and the expected value\n3. Use the loss to run the optimizer to update the weights and biases\n4. Call the visualization functions to visualize the training process",
      "Let's call the training method and pass in the model, training data, and optimizer."
    ],
    "code": "import numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport scienceplots\nfrom IPython.display import Image\n\nnp.random.seed(0)\n\n# ---\n\ndef generate_function(dims):\n    a = 1\n    b = 1\n\n    # Hyperbolic Paraboloid\n    x = np.linspace(-1, 1, dims)\n    y = np.linspace(-1, 1, dims)\n    X, Y = np.meshgrid(x, y)\n    Z = (Y**2 / b**2) - (X**2 / a**2)\n\n    X_t = X.flatten()\n    Y_t = Y.flatten()\n    Z_t = Z.flatten()\n    X_t = X_t.reshape((len(X_t), 1))\n    Y_t = Y_t.reshape((len(Y_t), 1))\n    Z_t = Z_t.reshape((len(Z_t), 1))\n    features = np.stack((X_t, Y_t), axis=1)\n    labels = Z_t.reshape((len(Z_t), 1, 1))\n\n    return X, Y, Z, features, labels\n\ndims = 12\nX, Y, Z, features, labels = generate_function(dims)\n\n# Visualize the Hyperbolic Paraboloid\n\n# ---\n\ndef mse(y_true, y_pred):\n    return np.mean(np.power(y_true - y_pred, 2))\n\n# ---\n\ndef mse_prime(y_true, y_pred):\n    return 2 * (y_pred - y_true) / np.size(y_true)\n\n# ---\n\nfrom abc import ABC, abstractmethod\n\nclass Layer(ABC):\n    def __init__(self):\n        self.input = None\n        self.output = None\n        self.weights = None\n        self.bias = None\n\n    @abstractmethod\n    def forward(self, input):\n        pass\n\n    @abstractmethod\n    def backward(self, output_gradient, optimizer):\n        pass\n\n# ---\n\nclass Dense(Layer):\n    def __init__(self, input_neurons, output_neurons):\n        # Random Values from Normal Distribution\n        # self.weights = np.random.randn(output_neurons, input_neurons)\n        # self.bias = np.random.randn(output_neurons, 1)\n\n        # All Zeros\n        # self.weights = np.zeros((output_neurons, input_neurons))\n        # self.bias = np.zeros((output_neurons, 1))\n\n        # Xavier Initialization Uniform Distribution\n        limit = np.sqrt(6 / (input_neurons + output_neurons))\n        self.weights = np.random.uniform(\n            -limit, limit, size=(output_neurons, input_neurons)\n        )\n        self.bias = np.zeros((output_neurons, 1))\n\n    def forward(self, input):\n        self.input = input\n        return np.matmul(self.weights, self.input) + self.bias\n\n    def backward(self, output_gradient, optimizer):\n        # Calculate gradients\n        weights_gradient = np.matmul(output_gradient, self.input.T)\n        input_gradient = np.dot(self.weights.T, output_gradient)\n\n        # Update weights and biases\n        self.weights, self.bias = optimizer.backward(\n            self.weights, weights_gradient, self.bias, output_gradient\n        )\n        return input_gradient\n\n# ---\n\nclass Activation(Layer):\n    def __init__(self, activation, activation_prime):\n        self.activation = activation\n        self.activation_prime = activation_prime\n\n    def forward(self, input_val):\n        self.input = input_val\n        return self.activation(self.input)\n\n    def backward(self, output_gradient, optimizer):\n        return np.multiply(output_gradient, self.activation_prime(self.input))\n\n    def plot(self, x_min, x_max, points=25):\n        x = np.linspace(x_min, x_max, points)\n        y = self.activation(x)\n        y_prime = self.activation_prime(y)\n\n        axes[0].plot(x, y)\n        axes[0].set_xlabel(\"X\")\n        axes[0].set_ylabel(\"Y\")\n        axes[0].set_title(\"F(X)\")\n\n        axes[1].plot(x, y_prime)\n        axes[1].set_xlabel(\"X\")\n        axes[1].set_ylabel(\"Y\")\n        axes[1].set_title(\"F'(X)\")\n\n# ---\n\nclass Tanh(Activation):\n    def __init__(self):\n        tanh = lambda x: np.tanh(x)\n        tanh_prime = lambda x: 1 - np.tanh(x) ** 2\n        super().__init__(tanh, tanh_prime)\n\nTanh().plot(-3, 3)\n\n# ---\n\nclass GradientDescentOptimizier:\n    def __init__(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def backward(self, weights, weights_gradient, bias, output_gradient):\n        weights -= self.learning_rate * weights_gradient\n        bias -= self.learning_rate * output_gradient\n\n        return weights, bias\n\n# ---\n\nimport copy\n\ndef create_scatter_and_3d_plot():\n\n    ax[0].set_xlabel(\"Epoch\", fontweight=\"normal\")\n    ax[0].set_ylabel(\"Error\", fontweight=\"normal\")\n    ax[0].set_title(\"Mean Squared Error\")\n\n    ax[1].axis(\"off\")\n    ax[2].axis(\"off\")\n\n    ax[2].set_xlabel(\"X\")\n    ax[2].set_ylabel(\"Y\")\n    ax[2].set_zlabel(\"Z\")\n    ax[2].set_title(\"Function Approximation\")\n    ax[2].view_init(20, -35)\n    ax[2].set_zlim(-1, 1)\n    ax[2].axis(\"equal\")\n\n    return ax[0], ax[2], camera\n\ndef create_3d_and_3d_plot():\n        1, 2, figsize=(16 / 9.0 * 4, 4 * 1), subplot_kw={\"projection\": \"3d\"}\n    )\n\n    ax[0].set_xlabel(\"W3_1\")\n    ax[0].set_ylabel(\"W3_2\")\n    ax[0].set_zlabel(\"MSE\")\n    ax[0].set_title(\"Mean Squared Error\")\n    ax[0].view_init(20, -35)\n    ax[0].set_zlim(-1, 1)\n    ax[0].axis(\"equal\")\n\n    ax[1].set_xlabel(\"X\")\n    ax[1].set_ylabel(\"Y\")\n    ax[1].set_zlabel(\"Z\")\n    ax[1].set_title(\"Function Approximation\")\n    ax[1].view_init(20, -35)\n    ax[1].set_zlim(-1, 1)\n    ax[1].axis(\"equal\")\n\n    return ax[0], ax[1], camera\n\ndef plot_3d_predictions(ax, X, Y, Z, predictions, dims):\n    # Plot Neural Network Function Approximation\n    # Ground truth\n        X, Y, Z, color=\"red\", alpha=0.5, label=\"Ground Truth\"\n    )\n\n    # Neural Network Predictions\n        X,\n        Y,\n        predictions.reshape((dims, dims)),\n        color=\"blue\",\n        alpha=0.2,\n        label=\"Prediction\",\n    )\n        X,\n        Y,\n        predictions.reshape((dims, dims)),\n        color=\"blue\",\n        alpha=0.3,\n    )\n        (ground_truth_legend, predictions_legend),\n        (\"Ground Truth\", \"Predictions\"),\n        loc=\"upper left\",\n    )\n\ndef plot_layer_loss_landscape(\n    ax0,\n    network,\n    target_layer_idx,\n    features,\n    labels,\n    w1_min,\n    w1_max,\n    w2_min,\n    w2_max,\n    loss_dims,\n):\n    # current target layer weights\n    target_layer_idx = target_layer_idx % len(network)\n\n    w1 = network[target_layer_idx].weights[0][0]\n    w2 = network[target_layer_idx].weights[0][1]\n    curr_error = 0\n    for x, y in zip(features, labels):\n        output = x\n        for layer in network:\n            output = layer.forward(output)\n\n        curr_error += mse(y, output)\n    curr_error /= labels.size\n    ax0.scatter([w1], [w2], [curr_error], color=\"red\", alpha=0.4)\n\n    target_layer = copy.deepcopy(network[target_layer_idx])\n    w1_range = np.linspace(w1_min, w1_max, loss_dims)\n    w2_range = np.linspace(w2_min, w2_max, loss_dims)\n    w1_range, w2_range = np.meshgrid(w1_range, w2_range)\n    w_range = np.stack((w1_range.flatten(), w2_range.flatten()), axis=1)\n\n    error_range = np.array([])\n\n    for target_layer_weight in w_range:\n        target_layer_weight = target_layer_weight.reshape(1, 2)\n        target_layer.weights[0, :2] = target_layer_weight[0, :2]\n\n        error = 0\n        for x, y in zip(features, labels):\n            output = x\n            for layer_idx, layer in enumerate(network):\n                if layer_idx == target_layer_idx:\n                    output = target_layer.forward(output)\n                else:\n                    output = layer.forward(output)\n\n            error += mse(y, output)\n        error /= labels.size\n        error_range = np.append(error_range, error)\n\n        w1_range,\n        w2_range,\n        error_range.reshape(loss_dims, loss_dims),\n        color=\"blue\",\n        alpha=0.1,\n    )\n\ndef plot_mse_and_predictions(\n    ax0, ax1, idx, visible_mse, mse_idx, errors, X, Y, Z, predictions, dims\n):\n    ax0.plot(\n        mse_idx[visible_mse][: idx + 1],\n        errors[visible_mse][: idx + 1],\n        color=\"red\",\n        alpha=0.5,\n    )\n\n    plot_3d_predictions(ax1, X, Y, Z, predictions, dims)\n\ndef plot_loss_landscape_and_predictions(\n    ax0,\n    ax1,\n    network,\n    target_layer_idx,\n    features,\n    labels,\n    X,\n    Y,\n    Z,\n    predictions,\n    preds_dims,\n    w1_min=-5,\n    w1_max=5,\n    w2_min=-5,\n    w2_max=5,\n    loss_dims=20,\n):\n    plot_3d_predictions(ax1, X, Y, Z, predictions, preds_dims)\n    plot_layer_loss_landscape(\n        ax0,\n        network,\n        target_layer_idx,\n        features,\n        labels,\n        w1_min,\n        w1_max,\n        w2_min,\n        w2_max,\n        loss_dims,\n    )\n\ndef show_epoch(epoch):\n    return (\n        epoch < 25\n        or (epoch < 25 and epoch % 2 == 0)\n        or (epoch <= 100 and epoch % 10 == 0)\n        or (epoch <= 500 and epoch % 25 == 0)\n        or (epoch <= 1000 and epoch % 50 == 0)\n        or epoch % 250 == 0\n    )\n\n# ---\n\ndef fit(\n    network,\n    features,\n    labels,\n    X,\n    Y,\n    Z,\n    preds_dims,\n    epochs,\n    optimizer,\n    mse_plot_filename,\n    loss_landscape_plot_filename,\n):\n    mse_idx = np.arange(1, epochs + 1)\n    errors = np.full(epochs, -1)\n    mse_ax, predictions_ax1, camera1 = create_scatter_and_3d_plot()\n    loss_landscape_ax, predictions_ax2, camera2 = create_3d_and_3d_plot()\n\n    network_len = len(network)\n\n    for idx in range(epochs):\n        error = 0\n        predictions = np.array([])\n\n        for x, y in zip(features, labels):\n            # Forward Propagation\n            output = x\n            for layer in network:\n                output = layer.forward(output)\n\n            predictions = np.append(predictions, output)\n\n            # Store Error\n            # no need to convert to numpy cpu, since both are tensors on device\n            error += mse(y, output)\n\n            # Backpropagation\n            grad = mse_prime(y, output)\n            for layer in reversed(network):\n                grad = layer.backward(grad, optimizer)\n\n        error /= len(X)\n\n        if show_epoch(idx):\n            print(f\"epoch: {idx}, MSE: {error}\")\n\n            # Plot MSE\n            errors[idx] = error\n            visible_mse = errors != -1\n            plot_mse_and_predictions(\n                mse_ax,\n                predictions_ax1,\n                idx,\n                visible_mse,\n                mse_idx,\n                errors,\n                X,\n                Y,\n                Z,\n                predictions,\n                preds_dims,\n            )\n\n            # plot the loss landscape of the second to last layer\n            # a 3D plot can be made because it's only 2 neurons\n            plot_loss_landscape_and_predictions(\n                loss_landscape_ax,\n                predictions_ax2,\n                network,\n                -2,\n                features,\n                labels,\n                X,\n                Y,\n                Z,\n                predictions,\n                preds_dims,\n            )\n\n            camera1.snap()\n            camera2.snap()\n\n# ---\n\nmodel = [Dense(2, 12), Tanh(), Dense(12, 2), Tanh(), Dense(2, 1), Tanh()]\n\n# ---\n\nepochs = 301\nlearning_rate = 0.005\noptimizer = GradientDescentOptimizier(learning_rate)\n\nmse_plot_filename = \"neural_network.gif\"\nloss_landscape_plot_filename = \"neural_network_loss_landscape.gif\"\nfit(\n    model,\n    features,\n    labels,\n    X,\n    Y,\n    Z,\n    dims,\n    epochs,\n    optimizer,\n    mse_plot_filename,\n    loss_landscape_plot_filename,\n)\n\n# ---\n\nImage(filename=mse_plot_filename)\n\n# ---\n\nImage(filename=loss_landscape_plot_filename)\n\n# ---\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(0)\n\n# ---\n\nclass TorchNet(nn.Module):\n    def __init__(self):\n        super(TorchNet, self).__init__()\n\n        # define the layers\n        self.fc1 = nn.Linear(2, 14)\n        self.fc2 = nn.Linear(14, 2)\n        self.fc3 = nn.Linear(2, 1)\n\n    def forward(self, x):\n        # pass the result of the previous layer to the next layer\n        x = F.tanh(self.fc1(x))\n        x = F.tanh(self.fc2(x))\n        return F.tanh(self.fc3(x))\n\n# ---\n\ndef torch_fit(\n    model, features, labels, X, Y, Z, dims, epochs, optimizer, output_filename\n):\n    mse_idx = np.arange(1, epochs + 1)\n    errors = np.full(epochs, -1)\n    mse_ax, predictions_ax1, camera1 = create_scatter_and_3d_plot()\n\n    loss_fn = nn.MSELoss()\n\n    for idx in range(epochs):\n        error = 0\n        predictions = np.array([])\n\n        for x, y in zip(features, labels):\n            # Forward Propagation\n            output = model(x)\n\n            output_np = output.detach().cpu().numpy()\n            predictions = np.append(predictions, output_np)\n\n            # Store Error\n            loss = loss_fn(output, y)\n\n            error += loss.detach().cpu().numpy()\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        error /= len(X)\n\n        if show_epoch(idx):\n            print(f\"epoch: {idx}, MSE: {error}\")\n\n            # Plot MSE\n            errors[idx] = error\n            visible_mse = errors != -1\n\n            plot_mse_and_predictions(\n                mse_ax,\n                predictions_ax1,\n                idx,\n                visible_mse,\n                mse_idx,\n                errors,\n                X,\n                Y,\n                Z,\n                predictions,\n                dims,\n            )\n\n            camera1.snap()\n\n# ---\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch_model = TorchNet().to(device)\n\n# the inputs and outputs for PyTorch must be tensors\nfeatures_tensor = torch.tensor(features, device=device, dtype=torch.float32).squeeze(-1)\nlabels_tensor = torch.tensor(labels, device=device, dtype=torch.float32).squeeze(-1)\n\nepochs = 101\nlearning_rate = 0.005\noptimizer = torch.optim.SGD(torch_model.parameters(), lr=learning_rate, momentum=0.0)\n\noutput_filename_pytorch = \"neural_network_pytorch.gif\"\ntorch_fit(\n    torch_model,\n    features_tensor,\n    labels_tensor,\n    X,\n    Y,\n    Z,\n    dims,\n    epochs,\n    optimizer,\n    output_filename_pytorch,\n)\n\n# ---\n\nImage(filename=output_filename_pytorch)",
    "tags": [],
    "visualization_data": {},
    "papers": [],
    "resources": []
  },
  {
    "name": "Perceptron",
    "repo": "perceptron",
    "category_name": "线性模型",
    "difficulty": "beginner",
    "description": "# Perceptron\n\nThe perceptron algorithm finds the optimal weights for a hyperplane to separate two classes, which is also known as binary classification.",
    "theory": [
      "Import the libraries",
      "## Training Dataset\n\nLet's generate a dataset where the label is determined by a linear decision boundary. Our perceptron will layer find the weights of a normal vector to separate the dataset into two classes.",
      "## Hyperplane\nA hyperplane is a flat subspace that is one less dimension than the current space. It can be used to linearly separate a dataset. The equation of a hyperplane is defined by the vector normal to the hyperplane $\\vec{w}$\n\n$\n\\begin{align*}\n\\vec{w} \\cdot \\vec{x} = w_1 x_1 + ... + w_n x_n = 0\n\\end{align*}\n$\n\nIn our case, the $\\vec{x}$ is the x, y, z coordinate.\n\n$\n\\begin{align*}\n\\vec{w} \\cdot \\vec{x} &= 0 \\\\\n&= w_1 x + w_2 y + w_3 z\n\\end{align*}\n$\n\nSince we want to perform binary classification using the side a point is on relative from the hyperplane, the z value can be our predicted label\n\n$\n\\begin{align*}\nz = -(w_1 x + w_2 y) / w_3\n\\end{align*}\n$",
      "## Loss Function: Hinge Loss\nLoss functions are used to quantify the error of a prediction.\n\nThe perceptron uses the hinge loss function, which returns 0 for correct predictions and 1 for incorrect predictions. \n\n$\n\\begin{align*}\nL(\\vec{w}, b) = max(0, -y(\\vec{w} \\cdot \\vec{x} + b)\n\\end{align*}\n$",
      "## Hinge Loss Gradient\n\nIn order to run gradient descent to update our parameters, the gradients with respect to W and b must be calculated",
      "## Hinge Loss Gradient in Terms of B\n\nLoss function with respect to $b$\n\n$\n\\frac{\\partial L}{\\partial b} = \\begin{cases}\n0 & -y(\\vec{w} \\cdot \\vec{x} + b) > 1 \\\\\n-y & \\text{otherwise}\n\\end{cases}\n$",
      "## Hinge Loss Gradient in Terms of W\n\nLoss function with respect to $\\vec{w}$\n\n$\n\\nabla_{W} [L(\\vec{w}, b)] = \\begin{cases}\n0 & -y(\\vec{w} \\cdot \\vec{x} + b) > 1 \\\\\n-y x & \\text{otherwise}\n\\end{cases}\n$",
      "## Graphing functions\n\nUtility functions to create the visualizations",
      "## Gradient Descent\n\nGradient Descent will be used to update the weights and bias.",
      "Bias Gradient Descent:\n\n$\n\\begin{align*}\nb &= b - \\eta \\frac{\\partial}{\\partial b} [L(\\vec{w}, b)] \\\\\n&= b - \\eta \\begin{cases}\n0 & -y(\\vec{w} \\cdot \\vec{x} + b) > 1 \\\\\n-y & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n$",
      "Weights Gradient Descent:\n\n$\n\\begin{align*}\n\\vec{w} &= \\vec{w} - \\eta \\nabla_{W} [L(\\vec{w}, b)] \\\\\n&= \\vec{w} - \\eta \\begin{cases}\n0 & -y(\\vec{w} \\cdot \\vec{x} + b) > 1 \\\\\n-y x & \\text{otherwise}\n\\end{cases}\n\\end{align*}\n$",
      "## Training the Model",
      "Let's put everything together and train our Perceptron",
      "## Output GIF"
    ],
    "code": "import numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport scienceplots\nfrom IPython.display import Image\n\nnp.random.seed(0)\n\n# ---\n\ndef generate_dataset(dims, normal_vector):\n    # create 3D grid of points\n    points = np.linspace(-1, 1, dims)\n    X, Y, Z = np.meshgrid(points, points, points)\n\n    # features are the x, y, z coordinates\n    features = np.column_stack((X.ravel(), Y.ravel(), Z.ravel()))\n\n    # labels are the side each point is on the hyperplane\n    distances = np.dot(features, normal_vector)\n    labels = np.where(distances >= 0, 1, -1)\n    return X, Y, Z, features, labels\n\n# normalized normal vector\ntarget_normal_vector = np.array([1.0, 1.0, 1.0])\ntarget_normal_vector = target_normal_vector / np.linalg.norm(target_normal_vector)\n\nscaling = 5\nX, Y, Z, features, labels = generate_dataset(scaling, target_normal_vector)\n\n# plot the points\n\n# ---\n\ndef generate_hyperplane(scaling, normal_vector):\n    # create 2D points\n    points = np.linspace(-1, 1, scaling)\n    xx, yy = np.meshgrid(points, points)\n\n    # the z value is the defined by the hyperplane\n    zz = -(normal_vector[0] * xx + normal_vector[1] * yy) / normal_vector[2]\n    return xx, yy, zz\n\nxx, yy, zz = generate_hyperplane(scaling, target_normal_vector)\n\n# visualize the hyperplane\n\n# plot the hyperplane defined by the normal vector\n    0,\n    0,\n    0,\n    target_normal_vector[0],\n    target_normal_vector[1],\n    target_normal_vector[2],\n    color=\"green\",\n    length=1,\n    arrow_length_ratio=0.2,\n)\n\n# ---\n\ndef hinge_loss(w, x, b, y):\n    return max(0.0, -y * (np.dot(w, x) + b))\n\n# ---\n\ndef hinge_loss_db(w, x, b, y):\n    if y * (np.dot(w, x) + b) <= 0.0:\n        return -y\n    return 0\n\n# ---\n\ndef hinge_loss_dw(w, x, b, y):\n    if y * (np.dot(w, x) + b) <= 0.0:\n        return -y * x\n    return np.zeros_like(x)\n\n# ---\n\ndef create_plots():\n\n    ax[0, 0].set_xlabel(\"Epoch\", fontweight=\"normal\")\n    ax[0, 0].set_ylabel(\"Error\", fontweight=\"normal\")\n    ax[0, 0].set_title(\"Hinge Loss\")\n\n    ax[1, 0].set_xlabel(\"Z, Distance to Hyperplane\", fontweight=\"normal\")\n    ax[1, 0].set_ylabel(\"\", fontweight=\"normal\")\n    ax[1, 0].set_title(\"Linear Transformation\")\n\n    ax[0, 1].axis(\"off\")\n    ax[0, 2].axis(\"off\")\n    ax[1, 1].axis(\"off\")\n    ax[1, 2].axis(\"off\")\n\n    ax[1, 2].set_xlabel(\"X\")\n    ax[1, 2].set_ylabel(\"Y\")\n    ax[1, 2].set_zlabel(\"Z\")\n    ax[1, 2].set_title(\"Hyperplane Decision Boundary\")\n    ax[1, 2].view_init(20, -35)\n    ax[1, 2].set_xlim(-1, 1)\n    ax[1, 2].set_ylim(-1, 1)\n    ax[1, 2].set_zlim(-1, 1)\n\n    return ax[0, 0], ax[1, 0], ax[1, 2], camera\n\n    ax0,\n    ax1,\n    ax2,\n    idx,\n    visible_err,\n    err_idx,\n    errors,\n    scaling,\n    target_normal_vector,\n    predictions,\n    features,\n    labels,\n    weights,\n):\n    ax0.plot(\n        err_idx[visible_err][: idx + 1],\n        errors[visible_err][: idx + 1],\n        color=\"red\",\n    )\n\n    # Ground truth\n    xx_target, yy_target, zz_target = generate_hyperplane(scaling, target_normal_vector)\n        xx_target,\n        yy_target,\n        zz_target,\n        color=\"red\",\n        alpha=0.2,\n        label=\"Ground Truth\",\n    )\n    ax2.quiver(\n        0,\n        0,\n        0,\n        target_normal_vector[0],\n        target_normal_vector[1],\n        target_normal_vector[2],\n        color=\"red\",\n        length=1,\n        arrow_length_ratio=0.1,\n    )\n\n    # Perceptron predictions using 2D graph to show linear transformation\n    def generate_colors(arr):\n        return [\"green\" if d >= 0 else \"orange\" for d in arr]\n\n    ground_truth_colors = generate_colors(labels)\n    ax1.scatter(\n        predictions,\n        np.zeros(predictions.shape),\n        c=ground_truth_colors,\n        marker=\"o\",\n        alpha=0.3,\n    )\n\n    # Perceptron predictions using 3D graph to show hyperplane\n    predictions_colors = generate_colors(predictions)\n    predictions_norm = np.maximum(1 - np.exp(-(predictions**2)), 0.2)\n\n    ax2.scatter(\n        features[:, 0],\n        features[:, 1],\n        features[:, 2],\n        c=predictions_colors,\n        marker=\"o\",\n        alpha=predictions_norm,\n    )\n\n    xx, yy, zz = generate_hyperplane(scaling, weights)\n        xx,\n        yy,\n        zz,\n        color=\"blue\",\n        alpha=0.2,\n        label=\"Prediction\",\n    )\n    ax2.quiver(\n        0,\n        0,\n        0,\n        weights[0],\n        weights[1],\n        weights[2],\n        color=\"blue\",\n        length=1,\n        arrow_length_ratio=0.1,\n    )\n\n    # Legend\n    ax2.legend(\n        (ground_truth_legend, predictions_legend),\n        (\"Ground Truth\", \"Predictions\"),\n        loc=\"upper left\",\n    )\n\n# ---\n\ndef gradient_descent(weights, x, bias, y, learning_rate):\n    weights = weights - learning_rate * hinge_loss_dw(weights, x, bias, y)\n    bias = bias - learning_rate * hinge_loss_db(weights, x, bias, y)\n\n    return weights, bias\n\n# ---\n\ndef fit(\n    weights,\n    bias,\n    target_normal_vector,\n    features,\n    labels,\n    X,\n    Y,\n    Z,\n    scaling,\n    epochs,\n    learning_rate,\n    optimizer,\n    output_filename,\n):\n    err_idx = np.arange(1, epochs + 1)\n    errors = np.full(epochs, -1)\n    ax0, ax1, ax2, camera = create_plots()\n\n    for idx in range(epochs):\n        error = 0\n        predictions = np.array([])\n\n        for x, y in zip(features, labels):\n            # Forward Propagation\n            output = np.dot(weights, x) + bias\n\n            predictions = np.append(predictions, output)\n\n            # Store Error\n            error += hinge_loss(weights, x, bias, y)\n\n            # Gradient Descent\n            weights, bias = optimizer(weights, x, bias, y, learning_rate)\n\n        error /= len(X)\n        weights = weights / np.linalg.norm(weights)\n\n        if (\n            idx < 5\n            or (idx < 15 and idx % 2 == 0)\n            or (idx <= 50 and idx % 10 == 0)\n            or (idx <= 1000 and idx % 20 == 0)\n            or idx % 250 == 0\n        ):\n\n            print(f\"epoch: {idx}, MSE: {error}\")\n\n            # Plot MSE\n            errors[idx] = error\n            visible_err = errors != -1\n\n                ax0,\n                ax1,\n                ax2,\n                idx,\n                visible_err,\n                err_idx,\n                errors,\n                scaling,\n                target_normal_vector,\n                predictions,\n                features,\n                labels,\n                weights,\n            )\n\n# ---\n\nweights = np.array([1.0, -1.0, -1.0])\nweights = weights / np.linalg.norm(weights)\n\nbias = 0\n\n# ---\n\nepochs = 301\nlearning_rate = 0.0005\n\noutput_filename = \"perceptron.gif\"\nfit(\n    weights,\n    bias,\n    target_normal_vector,\n    features,\n    labels,\n    X,\n    Y,\n    Z,\n    scaling,\n    epochs,\n    learning_rate,\n    gradient_descent,\n    output_filename,\n)\n\n# ---\n\nImage(filename=output_filename)",
    "tags": [],
    "visualization_data": {},
    "papers": [],
    "resources": []
  }
]